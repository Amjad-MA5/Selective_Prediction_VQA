{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Read Precomputed logits** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Coverage percentages at different risk%**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computed in risk_bounds notebook - check that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Calibration Curve**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "# Calculate calibration curve\n",
    "\n",
    "def plot_calibration(labels, predictions):\n",
    "    prob_true, prob_pred = calibration_curve(labels, predictions, n_bins=10)\n",
    "    # Plot calibration curve\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.plot(prob_pred, prob_true, marker='o', label='Logistic Regression')\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', label='Perfectly Calibrated')\n",
    "    plt.xlabel('Predicted Probability')\n",
    "    plt.ylabel('True Probability')\n",
    "    plt.title('Calibration Curve')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prediction(cali):\n",
    "    \"\"\"\n",
    "        Input: Calibration model\n",
    "        Output : prediction of uncalibrated and calibrated models + original_labels\n",
    "                [original_labels, uncalibrated_pred, calibrated_predictions]\n",
    "    \"\"\"\n",
    "    data_path = \"/teamspace/studios/this_studio/Selective_Prediction_VQA/predictions/logits_and_labels/\"\n",
    "    NUM_BATCH = 2139\n",
    "    original_labels = []\n",
    "    uncalibrated_pred = []\n",
    "    calibrated_pred = []\n",
    "    softmax = torch.nn.Softmax(dim=1)\n",
    "    for batch_no in range(NUM_BATCH):\n",
    "        file_name = \"Logits_and_labels\" + str(batch_no) + \".pt\"\n",
    "        data = torch.load(data_path + file_name)\n",
    "        \n",
    "        for logits, labels in zip(data['logits'], data['labels']):\n",
    "            if(len(labels)) == 0:\n",
    "                continue\n",
    "            \n",
    "            logits_cal = cali.calibrate(logits)\n",
    "            \n",
    "            logits_max_prob = torch.from_numpy(logits)\n",
    "            logits_cal = torch.from_numpy(logits_cal)\n",
    "            \n",
    "            prob_max_prob =  softmax(logits_max_prob)\n",
    "            prob_vec_cal =  softmax(logits_cal)\n",
    "            \n",
    "            # print(prob)\n",
    "            # print(torch.max(prob).numpy())\n",
    "            # prob.to('cpu')\n",
    "            uncalibrated_pred.append(torch.argmax(prob_max_prob).numpy())\n",
    "            calibrated_pred.append(torch.argmax(prob_ve_cal).numpy())\n",
    "            original_labels.append(max_occurence(labels))\n",
    "            # idx = torch.argmax(prob)\n",
    "            # print(prob[0][idx])\n",
    "            # print(labels)\n",
    "            # residuals.append(idx.item() != max_occurence(labels))\n",
    "            # print(kappa)\n",
    "            # print(residuals)\n",
    "    return [original_labels, uncalibrated_pred, calibrated_pred]\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
